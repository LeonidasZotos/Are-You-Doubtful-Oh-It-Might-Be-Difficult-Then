{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This code is adapted from: https://github.com/victoria-ianeva/NERA-Tutorial-on-linguistic-feature-extraction***\n",
    "\n",
    "It extracts linguistic features from the datasets and then uses them to predict the target value (difficulty, facility or response time) for our 3 datasets(bea, bio, cmcqrd)\n",
    "***Readability metrics are not applied, as most question items have fewer than 100 words (https://github.com/cdimascio/py-readability-metrics)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import glob\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    '''This is a function to perform tokenization, lemmatization, removal of non-alphabetic characters\n",
    "    and stopword removal'''\n",
    "    stopwords = ['a', 'an', 'the', 'with', 'to', 'be', 'have']\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    return ' '.join(a_lemmas)\n",
    "\n",
    "def count_words(string):\n",
    "    '''This function returns the number of words in a string'''\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "  \n",
    "def word_length(string):\n",
    "    '''This function returns the average word length in characters for the words in an item'''\n",
    "    #Get the length of the full text in characters\n",
    "    chars = len(string)\n",
    "    #Split the string into words\n",
    "    words = string.split()\n",
    "    #Compute the average word length and round the output to the second decimal point\n",
    "    avg_word_length = chars/len(words)\n",
    "    return round(avg_word_length, 2)\n",
    "  \n",
    "def sentence_counter(text):\n",
    "    '''This function returns the number of sentences in an item'''\n",
    "    doc = nlp(text)\n",
    "    #Initialize a counter variable\n",
    "    counter = 0\n",
    "    #Update the counter for each sentence which can be found in the doc.sents object returned by the Spacy model\n",
    "    for sentence in doc.sents:\n",
    "        counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "def avg_sent_length(text):\n",
    "    '''This function returns the average sentence length in an item'''\n",
    "    doc = nlp(text)\n",
    "    #Initialize a counter variable\n",
    "    sent_number = 0\n",
    "    #Update the counter for each sentence which can be found in the doc.sents object returned by the Spacy model\n",
    "    for sent in doc.sents:\n",
    "        sent_number = sent_number + 1\n",
    "    #Get the number of words\n",
    "    words = text.split()\n",
    "    #Compute the average sentence length and round it to the second decimal point\n",
    "    avg_sent_length = len(words)/sent_number\n",
    "    return round(avg_sent_length, 2)\n",
    "\n",
    "def nouns(text, model=nlp):\n",
    "    '''This function returns the number of nouns in an item'''\n",
    "    # Create doc object \n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "def verbs(text, model=nlp):\n",
    "    '''This function returns the number of verbs in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of verbs\n",
    "    return pos.count('VERB')\n",
    "\n",
    "def adjectives(text, model=nlp):\n",
    "    '''This function returns the number of adjectives in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adjectives\n",
    "    return pos.count('ADJ')\n",
    "\n",
    "def adverbs(text, model=nlp):\n",
    "    '''This function returns the number of adverbs in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('ADV')\n",
    "\n",
    "def get_nps(text):\n",
    "    '''This is a function that outputs the number of noun phrases in an item'''\n",
    "    doc = nlp(text)\n",
    "    NP_count = 0\n",
    "    for np in doc.noun_chunks:\n",
    "        NP_count = NP_count + 1\n",
    "    return NP_count\n",
    "\n",
    "def get_pps(text):\n",
    "    '''This is a function that outputs the number of prepositional phrases in an item'''\n",
    "    doc = nlp(text)\n",
    "    pps = 0\n",
    "    for token in doc:\n",
    "        # You can try this with other parts of speech for different subtrees.\n",
    "        if token.pos_ == 'ADP':\n",
    "            \n",
    "            #Use the command below if you wanted to get the actual PPs\n",
    "            #pp = ' '.join([tok.orth_ for tok in token.subtree])\n",
    "            \n",
    "            #This command counts the number of PPs\n",
    "            pps = pps + 1\n",
    "    return pps\n",
    "\n",
    "def get_vps(text):\n",
    "    '''This function returns the number of verb phrases in an item'''\n",
    "    #We can modify this pattern for the extraction of verb phrases\n",
    "    pattern = [{'POS': 'VERB', 'OP': '?'},\n",
    "            {'POS': 'ADV', 'OP': '*'},\n",
    "            {'POS': 'AUX', 'OP': '*'},\n",
    "            {'POS': 'VERB', 'OP': '+'}]\n",
    "    doc = nlp(text)\n",
    "    vps = 0\n",
    "    # instantiate a Matcher instance\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", [pattern], on_match=None)\n",
    "    # call the matcher to find matches \n",
    "    matches = matcher(doc)\n",
    "    spans = [doc[start:end] for _, start, end in matches]\n",
    "    for match in matches:\n",
    "        vps = vps +1\n",
    "    return vps\n",
    "\n",
    "#Connectives to instruct, recount and sequence\n",
    "temporal_connectives = ['afterwards', 'once', 'at this moment', 'at this point', 'before', 'finally', \n",
    "                        'here', 'in the end', 'lastly', 'later on', 'meanwhile', 'next', 'now', \n",
    "                        'on another occasion', 'previously','since', 'soon', 'straightaway', 'then', \n",
    "                        'when', 'whenever', 'while']\n",
    "\n",
    "\n",
    "#Connectives to show cause or conditions\n",
    "causal_connectives = ['accordingly', 'all the same', 'an effect of', 'an outcome of', 'an upshot of',\n",
    "                      'as a consequence of', 'as a result of', 'because', 'caused by', 'consequently',\n",
    "                      'despite this', 'even though', 'hence', 'however', 'in that case', 'moreover',\n",
    "                      'nevertheless', 'otherwise', 'so', 'so as', 'stemmed from', 'still', 'then',\n",
    "                      'therefore', 'though', 'under the circumstances', 'yet']\n",
    "\n",
    "\n",
    "#Connectives for showing results\n",
    "exemplifying_connectives = ['accordingly', 'as a result', 'as exemplified by', 'consequently', 'for example',\n",
    "                            'for instance', 'for one thing', 'including', 'provided that', 'since', 'so',\n",
    "                            'such as', 'then', 'therefore', 'these include', 'through', 'unless', 'without']\n",
    "\n",
    "\n",
    "#Connectives to show similarity or add a point\n",
    "additive_connectives = ['and', 'additionally', 'also', 'as well', 'even', 'furthermore', 'in addition', 'indeed',\n",
    "                        'let alone', 'moreover', 'not only']\n",
    "\n",
    "#Connectives showing a difference or an opposite point of view\n",
    "contrastive_connectives = ['alternatively', 'anyway', 'but', 'by contrast', 'differs from', 'elsewhere',\n",
    "                           'even so', 'however', 'in contrast', 'in fact', 'in other respects', 'in spite of this',\n",
    "                           'in that respect', 'instead', 'nevertheless', 'on the contrary', 'on the other hand',\n",
    "                           'rather', 'though', 'whereas', 'yet']\n",
    "def temporal_connectives_count(text):\n",
    "    '''This function counts the number of temporal connectives in a text'''\n",
    "    count = 0\n",
    "    for string in temporal_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "def causal_connectives_count(text):\n",
    "    '''This function counts the number of causal connectives in a text'''\n",
    "    count = 0\n",
    "    for string in causal_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "def exemplifying_connectives_count(text):\n",
    "    '''This function counts the number of exemplifying connectives in a text'''\n",
    "    count = 0\n",
    "    for string in exemplifying_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "def additive_connectives_count(text):\n",
    "    '''This function counts the number of additive connectives in a text'''\n",
    "    count = 0\n",
    "    for string in additive_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "def contrastive_connectives_count(text):\n",
    "    '''This function counts the number of contrastive connectives in a text'''\n",
    "    cont_con = 0\n",
    "    for string in contrastive_connectives:\n",
    "        if string in text:\n",
    "            cont_con = cont_con + 1\n",
    "    return cont_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_all_functions(df):\n",
    "    print(\"Step 1\")\n",
    "    df['question_with_options'] = df['question_with_options'].str.lower()\n",
    "    df['Item'] = df['question_with_options'].apply(preprocess)\n",
    "    df['Word_Count'] = df['question_with_options'].apply(count_words)\n",
    "    df['Word_Count_No_stop_words'] = df['Item'].apply(count_words)\n",
    "    df['Avg_Word_Length'] = df['Item'].apply(word_length)\n",
    "    df['Sentence_Count'] = df['question_with_options'].apply(sentence_counter)\n",
    "    df['Avg_Sent_Length_in_Words'] = df['question_with_options'].apply(avg_sent_length)\n",
    "    print(\"Step 2\")\n",
    "    df['Noun_Count'] = df['Item'].apply(nouns)\n",
    "    df['Verb_Count'] = df['Item'].apply(verbs)\n",
    "    df['Adjective_Count'] = df['Item'].apply(adjectives)\n",
    "    df['Adverb_Count'] = df['Item'].apply(adverbs)\n",
    "    df['Number_of_NPs'] = df['Item'].apply(get_nps)\n",
    "    df['Number_of_PPs'] = df['Item'].apply(get_pps)\n",
    "    df['Number_of_VPs'] = df['Item'].apply(get_vps)\n",
    "    print(\"Step 3\")\n",
    "    df['Temporal_Connectives_Count'] = df['question_with_options'].apply(temporal_connectives_count)\n",
    "    df['Causal_Connectives_Count'] = df['question_with_options'].apply(causal_connectives_count)\n",
    "    df['Exemplifying_Connectives_Count'] = df['question_with_options'].apply(exemplifying_connectives_count)\n",
    "    df['Additive_Connectives_Count'] = df['question_with_options'].apply(additive_connectives_count)\n",
    "    df['Contrastive_Connectives_Count'] = df['question_with_options'].apply(contrastive_connectives_count)\n",
    "    print(\"Step 4\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply for all datasets and splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['bio','cmcqrd','usmle']\n",
    "SPLITS = ['train', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in DATASETS:\n",
    "    for split in SPLITS:\n",
    "        print(\"Dataset: \", dataset, \"Split: \", split)\n",
    "        df = pd.read_csv(f'../data/{dataset}/preprocessed/combined_results_{split}_set.csv')\n",
    "        df = apply_all_functions(df)\n",
    "        df.to_csv(f'../data/{dataset}/with_ling_features/{split}.csv', index=False)\n",
    "        \n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "difficulty_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
