{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.environ['HF_HOME'] = '/scratch/' + str(open('../tokens/HPC_ACCOUNT_ID.txt', 'r').read())\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MAIN_DF_NAME also dictates the values of \"full_answer\", \"full_prompt\", as we don't store the other models' full answers\n",
    "MAIN_DF_NAME = \"Qwen2_5-14b-chat\"\n",
    "MODEL_NAMES = ['phi3_5-chat', 'Llama3_2-3b-chat', 'Qwen2_5-3b-chat', 'Llama3_1-8b-chat', 'Qwen2_5-14b-chat', 'Qwen2_5-32b-chat', 'Yi-34b-chat', 'Llama3_1-70b-chat', 'Qwen2_5-72b-chat']\n",
    "\n",
    "HF_TOKEN = open('../tokens/HF_TOKEN.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_uncertainties(df, dataset_name, split_name, full_precision):\n",
    "    \"\"\"Combines the uncertainties of the different models into one dataframe\"\"\"\n",
    "    for cols in [\"first_token_probability\", \"order_probability\", \"first_token_probability_selected_choice\", \"order_probability_selected_choice\", \"full_answer\", \"model_is_correct\"]:\n",
    "        df = df.rename(columns={cols: cols + \"_\" + MAIN_DF_NAME})\n",
    "\n",
    "    # For each of the other dfs, we now add the uncertainty column to the main df\n",
    "    for extra_model_df_name in MODEL_NAMES:\n",
    "        df_extra = pd.read_csv(\"../data/\" + dataset_name + \"/with_uncertainty/\" + extra_model_df_name + \"_\" + full_precision + split_name + \"_set.csv\")\n",
    "        # We copy over the uncertainty of the other models, and we annotate the column as such\n",
    "        df[\"first_token_probability\" + \"_\" + extra_model_df_name] = df_extra[\"first_token_probability\"]\n",
    "        df[\"order_probability\" + \"_\" + extra_model_df_name] = df_extra[\"order_probability\"]\n",
    "        df[\"first_token_probability_selected_choice\" + \"_\" + extra_model_df_name] = df_extra[\"first_token_probability_selected_choice\"]\n",
    "        df[\"order_probability_selected_choice\" + \"_\" + extra_model_df_name] = df_extra[\"order_probability_selected_choice\"]\n",
    "        df[\"model_is_correct\" + \"_\" + extra_model_df_name] = df_extra[\"model_is_correct\"]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_with_options_string(df):\n",
    "    \"\"\"Creates a new column with the question and options only\"\"\"\n",
    "    # We add a column with the question and options only. We iterate through all rows, and for each row we add the question\n",
    "    # and options to the new column (making sure we don't include empty options)\n",
    "    choice_columns = ['Answer_' + chr(ord('A') + i)\n",
    "                        for i in range(10)]  # Answer_A ... Answer_J\n",
    "    choice_letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "    for row in df.iterrows():\n",
    "        choices = \"\"\n",
    "        for i in range(10):\n",
    "            if pd.notna(row[1][choice_columns[i]]):\n",
    "                choice_text = row[1][choice_columns[i]]\n",
    "                if choice_text:\n",
    "                    choices += choice_letters[i] + \") \" + str(choice_text) + \"\\n\"\n",
    "        df.at[row[0], 'question_with_options'] = row[1]['Question'] + '\\n' + choices\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_choice_similarity(df):\n",
    "    \"\"\"Calculates the average similarity between the answer and the choices\"\"\"\n",
    "    # Function to compute average similarity score\n",
    "    def compute_average_similarity(row):\n",
    "        # Generate embeddings\n",
    "        answer_text_embedding = model.encode([str(row['Answer_Text'])])\n",
    "        choices = [row['Answer_A'], row['Answer_B'], row['Answer_C'], row['Answer_D'], row['Answer_E'],\n",
    "                   row['Answer_F'], row['Answer_G'], row['Answer_H'], row['Answer_I'], row['Answer_J']]\n",
    "        choices = [choice for choice in choices if not pd.isna(choice)]\n",
    "        choices = [choice for choice in choices if choice != row['Answer_Text']]\n",
    "        choices_embeddings = model.encode(choices)\n",
    "\n",
    "        # Compute cosine similarity between answer text and each choice\n",
    "        similarities = cosine_similarity(\n",
    "            answer_text_embedding, choices_embeddings).flatten()\n",
    "        # Compute average similarity\n",
    "        mean_similarity = similarities.mean().item()\n",
    "        return mean_similarity\n",
    "\n",
    "\n",
    "    # Load pre-trained model for generating embeddings\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", token = HF_TOKEN) #Â efficient model\n",
    "    df['choices_similarity'] = df.apply(compute_average_similarity, axis=1)\n",
    "    \n",
    "    model = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO', token = HF_TOKEN) # bio clinical model\n",
    "    df['choices_similarity_clinical'] = df.apply(compute_average_similarity, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in ['usmle', 'bio', 'cmcqrd']:\n",
    "    for split in ['train', 'test']:\n",
    "        for full_precision in [\"fp_\", \"\"]:\n",
    "            print(\"Processing \" + dataset_name + \" \" + split)\n",
    "            input_file_name = \"../data/\" + dataset_name + \"/with_uncertainty/\" + MAIN_DF_NAME + \"_\" + full_precision  + split + \"_set.csv\"\n",
    "            output_file_name = \"../data/\" + dataset_name + \"/preprocessed/combined_results_\" + full_precision  + split +  \"_set.csv\"\n",
    "            \n",
    "            combined_df = combine_uncertainties(pd.read_csv(input_file_name), dataset_name, split, full_precision)\n",
    "            combined_df = create_question_with_options_string(combined_df)\n",
    "            combined_df = add_choice_similarity(combined_df)\n",
    "            \n",
    "            combined_df.to_csv(output_file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "difficulty_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
