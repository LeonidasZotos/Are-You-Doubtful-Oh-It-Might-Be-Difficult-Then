{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DATASET = 'usmle' # one of bio, usmle, cmcqrd\n",
    "FP = True\n",
    "\n",
    "fp = \"fp_\" if FP else \"\"\n",
    "# Load the train and test and combine them\n",
    "df1 = pd.read_csv(f'../data/{DATASET}/preprocessed/combined_results_{fp}train_set.csv')\n",
    "df2 = pd.read_csv(f'../data/{DATASET}/preprocessed/combined_results_{fp}test_set.csv')\n",
    "all_data = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Define model names and uncertainty metric names\n",
    "MODEL_NAMES = ['phi3_5-chat', 'Llama3_2-3b-chat', 'Qwen2_5-3b-chat',\n",
    "               'Llama3_1-8b-chat', 'Qwen2_5-14b-chat', 'Qwen2_5-32b-chat',\n",
    "               'Yi-34b-chat', 'Llama3_1-70b-chat', 'Qwen2_5-72b-chat']\n",
    "\n",
    "uncertainty_metrics = ['first_token_probability', 'order_probability']\n",
    "false_choice_uncertainty_metrics = ['first_token_probability_selected_choice', 'order_probability_selected_choice']\n",
    "\n",
    "\n",
    "latex_str = \"\"\n",
    "latex_str += \"\\\\multirow{9}{*}{\"\n",
    "if DATASET == 'bio':\n",
    "    latex_str += \"Biopsychology}\"\n",
    "elif DATASET == 'usmle':\n",
    "    latex_str += \"USMLE}\"\n",
    "elif DATASET == 'cmcqrd':\n",
    "    latex_str += \"CMCQRD}\"\n",
    "\n",
    "\n",
    "for idx, model in enumerate(MODEL_NAMES):\n",
    "    model_latex = model.replace('_', '\\\\_').replace('-chat', '')\n",
    "    \n",
    "    mean_first_token_correct = round(all_data.loc[all_data[f'model_is_correct_{model}'] == True, f'first_token_probability_{model}'].mean(), 3)\n",
    "    mean_order_correct       = round(all_data.loc[all_data[f'model_is_correct_{model}'] == True, f'order_probability_{model}'].mean(), 3)\n",
    "    \n",
    "    mean_first_token_incorrect = round(all_data.loc[all_data[f'model_is_correct_{model}'] == False, f'first_token_probability_selected_choice_{model}'].mean(), 3)\n",
    "    mean_order_incorrect       = round(all_data.loc[all_data[f'model_is_correct_{model}'] == False, f'order_probability_selected_choice_{model}'].mean(), 3)\n",
    "    \n",
    "    overall_correctness = round(all_data[f'model_is_correct_{model}'].sum() / len(all_data), 3)\n",
    "    \n",
    "    if idx == 0:\n",
    "        row = f\" & {model_latex} & {overall_correctness} & \\\\textcolor{{ForestGreen}}{{{mean_first_token_correct}}} / \\\\textcolor{{BrickRed}}{{{mean_first_token_incorrect}}} & \\\\textcolor{{ForestGreen}}{{{mean_order_correct}}} / \\\\textcolor{{BrickRed}}{{{mean_order_incorrect}}} \\\\\\\\\\n\"\n",
    "    else:\n",
    "        row = f\"                               & {model_latex} & {overall_correctness} & \\\\textcolor{{ForestGreen}}{{{mean_first_token_correct}}} / \\\\textcolor{{BrickRed}}{{{mean_first_token_incorrect}}} & \\\\textcolor{{ForestGreen}}{{{mean_order_correct}}} / \\\\textcolor{{BrickRed}}{{{mean_order_incorrect}}} \\\\\\\\\\n\"\n",
    "    latex_str += row\n",
    "\n",
    "# Print the LaTeX table code\n",
    "print(latex_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "difficulty_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
