{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "# Sci-kit learn packages\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Packages for results/plotting\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import ticker\n",
    "from shap import summary_plot, TreeExplainer\n",
    "\n",
    "# Change default font of matplotlib to monospace\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rcParams[\"font.family\"] = \"monospace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cmcqrd' # 'usmle', 'bio' or 'cmcqrd'\n",
    "RESULTS_DATASET = '../data/' + DATASET + '/preprocessed/combined_results'\n",
    "TARGET_LABEL_COL_NAME = 'Correct_Answer_Rate' # \"Correct_Answer_Rate\" or \"difficulty\"(only for cmcqrd) or \"Response_Time\"(for usmle only)\n",
    "REPETITIONS = 10  # Number of repetitions for each experiment, average will be taken\n",
    "# Number of cores to use for sklearn's n_jobs parameter, whenever possible\n",
    "NUM_OF_CORES_TO_USE = multiprocessing.cpu_count()\n",
    "print(\"Using \", NUM_OF_CORES_TO_USE, \" cores.\")\n",
    "# TF-IDF threshold for feature selection\n",
    "TFIDF_THRESHOLD = 0.0007  # best found: 0.0007, higher->fewer features\n",
    "# Number of most important features to print for the random forest model\n",
    "NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT = 10\n",
    "# Define the feature columns from the other models, that contain other uncertainty features\n",
    "MODEL_NAMES = ['phi3_5-chat', 'Llama3_2-3b-chat', 'Qwen2_5-3b-chat', 'Llama3_1-8b-chat', 'Qwen2_5-14b-chat', 'Qwen2_5-32b-chat', 'Yi-34b-chat', 'Qwen2_5-72b-chat', 'Llama3_1-70b-chat']\n",
    "\n",
    "# Which experiments to run:\n",
    "FULL_PRECISION_MODELS = True\n",
    "CHOICE_SIMILARITY_EXPERIMENTS = True\n",
    "\n",
    "# Prepare export folders/make sure they are there. \n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')\n",
    "if not os.path.exists('plots/' + DATASET):\n",
    "    os.makedirs('plots/' + DATASET)\n",
    "if not os.path.exists('plots/' + DATASET + '/SHAP_summary'):\n",
    "    os.makedirs('plots/' + DATASET + '/SHAP_summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = None\n",
    "test = None\n",
    "\n",
    "if FULL_PRECISION_MODELS:\n",
    "    train = pd.read_csv(RESULTS_DATASET + '_fp_train_set.csv')\n",
    "    test = pd.read_csv(RESULTS_DATASET + '_fp_test_set.csv', index_col=0)\n",
    "else:\n",
    "    train = pd.read_csv(RESULTS_DATASET + '_train_set.csv')\n",
    "    test = pd.read_csv(RESULTS_DATASET + '_test_set.csv', index_col=0)\n",
    "\n",
    "\n",
    "train_with_linguistic = pd.read_csv('../data/' + DATASET + '/with_ling_features/' + 'train.csv')\n",
    "test_with_linguistic = pd.read_csv('../data/' + DATASET + '/with_ling_features/' + 'test.csv', index_col=0)\n",
    "\n",
    "# Size per split\n",
    "print(\"Train size: \", len(train))\n",
    "print(\"Test size: \", len(test))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF-IDF features from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TF-IDF scores\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),  # Use 1-grams and 2-grams.\n",
    "    # Ignore terms that appear in less than 0.1% of the documents.\n",
    "    min_df=0.001,\n",
    "    # Ignore terms that appear in more than 75% of documents.\n",
    "    max_df=0.75,\n",
    "    max_features=1000,  # Use only the top 1000 most frequent words.\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "textTrain = vectorizer.fit_transform(train['question_with_options']).toarray()\n",
    "textTest = vectorizer.transform(test['question_with_options']).toarray()\n",
    "\n",
    "textTrain = pd.DataFrame(\n",
    "    textTrain,\n",
    "    columns=['\"' + w + '\"' for w in vectorizer.get_feature_names_out()],\n",
    "    index=train.index\n",
    ")\n",
    "\n",
    "textTest = pd.DataFrame(\n",
    "    textTest,\n",
    "    columns=['\"' + w + '\"' for w in vectorizer.get_feature_names_out()],\n",
    "    index=test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust threshold as needed, the lower the threshold, the more features will be selected.\n",
    "selector = VarianceThreshold(threshold=TFIDF_THRESHOLD)\n",
    "textTrain_selected = selector.fit_transform(textTrain)\n",
    "textTrain_selected = pd.DataFrame(textTrain_selected,\n",
    "                                  columns=textTrain.columns[selector.get_support(\n",
    "                                  )],\n",
    "                                  index=textTrain.index)\n",
    "\n",
    "# Apply to test set\n",
    "textTest_selected = selector.transform(textTest)\n",
    "textTest_selected = pd.DataFrame(textTest_selected,\n",
    "                                 columns=textTrain.columns[selector.get_support(\n",
    "                                 )],\n",
    "                                 index=textTest.index)\n",
    "\n",
    "print(textTrain_selected.shape)\n",
    "textTrain = textTrain_selected\n",
    "textTest = textTest_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_ALL_RESULTS = {} # Store all results here\n",
    "\n",
    "def test_random_forest(features_train, target_train, features_test, target_test, description):\n",
    "    \"\"\"Runs a Random Forest model on the given data and returns the RMSE and the top features. Average of REPETITIONS is taken.\"\"\"\n",
    "    feature_importances_sum = None\n",
    "\n",
    "    all_rmses_for_model = []\n",
    "\n",
    "    # Initialize a sum array for feature importances\n",
    "    feature_importances_sum = np.zeros(features_train.shape[1])  # Number of features in the dataset)\n",
    "    shap_values = []\n",
    "    for repetition in tqdm(range(REPETITIONS)):\n",
    "        model = RandomForestRegressor(n_jobs=NUM_OF_CORES_TO_USE) # Re-initialize the model every time\n",
    "        # Fit the random forest. model.fit resets the model every time so it doesn't remember the previous fit.\n",
    "        model.fit(features_train, target_train)\n",
    "        # Predict the target values\n",
    "        predictions = model.predict(features_test)\n",
    "        rmse = root_mean_squared_error(predictions, target_test)  # Calculate RMSE\n",
    "        all_rmses_for_model.append(rmse)\n",
    "        # Collect feature importances\n",
    "        feature_importances_sum += model.feature_importances_\n",
    "        ### SHAP\n",
    "        explainer = TreeExplainer(model, approximate=True)\n",
    "        shap_values_of_repetition = explainer(features_train)\n",
    "        shap_values.append(shap_values_of_repetition)\n",
    "\n",
    "\n",
    "    # Calculate statistics for the model\n",
    "    average_rmse_for_model = float(np.mean(all_rmses_for_model)) # Mean RMSE\n",
    "    std_dev_rmse = float(np.std(all_rmses_for_model, ddof=1))  # Sample standard deviation\n",
    "    std_error_rmse = float(std_dev_rmse / np.sqrt(REPETITIONS))  # Standard error\n",
    "\n",
    "    # Store a summary of the results\n",
    "    rmse_results_summary = {\n",
    "        'rmse': round(average_rmse_for_model, 4),\n",
    "        'std_dev': round(std_dev_rmse, 4),\n",
    "        'std_error': round(std_error_rmse, 4)\n",
    "    }\n",
    "\n",
    "    # Print the top NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT important features for Random Forest\n",
    "    # Calculate the average feature importances across all repetitions\n",
    "    avg_feature_importances = feature_importances_sum / REPETITIONS\n",
    "    # Get the indices of the top NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT most important features\n",
    "    indices = np.argsort(avg_feature_importances)[-NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT:][::-1]\n",
    "    top_features_and_importances = [(features_train.columns[i], float(round(avg_feature_importances[i], 4))) for i in indices]\n",
    "    \n",
    "    # Average SHAP values over repetitions\n",
    "    shap_values = np.array([shap_values[i].values for i in range(REPETITIONS)])\n",
    "    shap_values = np.mean(shap_values, axis=0)\n",
    "    \n",
    "    # Summary plot:\n",
    "    summary_plot(shap_values, features_train, max_display=NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT, plot_size=[12, 7], show=False)\n",
    "    plt.suptitle('Effect of Top Features on Predicting Student Success \\n(' + description + ')', fontsize=20, x=0.5, y=1.1)\n",
    "    # change font size of the ticks\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.xlabel('Impact on Random Forest Prediction', fontsize=18)\n",
    "    ax = plt.gca()  # Get current axis\n",
    "    ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f\"{x:.2g}\"))\n",
    "\n",
    "    \n",
    "    plt.savefig(f'plots/{DATASET}/SHAP_summary/{DATASET}_shap_summary_' + description + '.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()    \n",
    "    \n",
    "    return rmse_results_summary, top_features_and_importances\n",
    "\n",
    "\n",
    "def retrieve_models_uncertainties_col_names(metric_names):\n",
    "    uncertainty_feature_columns = []\n",
    "    for metric in metric_names:\n",
    "        for model in MODEL_NAMES:\n",
    "            uncertainty_feature_columns.append(f'{metric}_{model}')\n",
    "    return uncertainty_feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Regressor (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the features\n",
    "features_train = pd.concat([textTrain], axis=1)\n",
    "target_train = train[TARGET_LABEL_COL_NAME]\n",
    "features_test = pd.concat([textTest], axis=1)\n",
    "target_test = test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "# Fit the dummy regressor\n",
    "dummy_regressor = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regressor.fit(features_train, target_train)\n",
    "\n",
    "# Predict the target values\n",
    "dummy_regressor_predictions = dummy_regressor.predict(features_test)\n",
    "rmse = float(root_mean_squared_error(target_test, dummy_regressor_predictions)) # float converts to normal float, not numpy float\n",
    "\n",
    "GLOBAL_ALL_RESULTS['DummyRegressor'] = ({'rmse': round(rmse, 4), 'std_dev': 0, 'std_error': 0}, [], None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, target_train, features_test, target_test = None, None, None, None\n",
    "# Concatenate the features\n",
    "features_train = pd.concat([textTrain], axis=1)\n",
    "target_train = train[TARGET_LABEL_COL_NAME]\n",
    "features_test = pd.concat([textTest], axis=1)\n",
    "target_test = test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "description = \"Only TF-IDF Features\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only First Token Probability (all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_feature_columns = retrieve_models_uncertainties_col_names(['first_token_probability'])\n",
    "\n",
    "# Concatenate the features\n",
    "ensemble_cols_train = pd.concat([train[col] for col in uncertainty_feature_columns], axis=1)\n",
    "ensemble_cols_test = pd.concat([test[col] for col in uncertainty_feature_columns], axis=1)\n",
    "\n",
    "features_train = pd.concat([ensemble_cols_train], axis=1)\n",
    "features_test = pd.concat([ensemble_cols_test], axis=1)\n",
    "\n",
    "target_train, target_test = train[TARGET_LABEL_COL_NAME], test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "description = \"All Models' 1st Token Probabilities\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Choice Order Probability  (all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_feature_columns = retrieve_models_uncertainties_col_names(['order_probability'])\n",
    "\n",
    "# Concatenate the features\n",
    "ensemble_cols_train = pd.concat([train[col] for col in uncertainty_feature_columns], axis=1)\n",
    "ensemble_cols_test = pd.concat([test[col] for col in uncertainty_feature_columns], axis=1)\n",
    "\n",
    "features_train = pd.concat([ensemble_cols_train], axis=1)\n",
    "features_test = pd.concat([ensemble_cols_test], axis=1)\n",
    "\n",
    "target_train, target_test = train[TARGET_LABEL_COL_NAME], test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "description = \"All Models' Choice Order Probabilities\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Token Probability and Choice Order Probability  (all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_feature_columns = retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])\n",
    "\n",
    "# Concatenate the features\n",
    "ensemble_cols_train = pd.concat([train[col] for col in uncertainty_feature_columns], axis=1)\n",
    "ensemble_cols_test = pd.concat([test[col] for col in uncertainty_feature_columns], axis=1)\n",
    "\n",
    "features_train = pd.concat([ensemble_cols_train], axis=1)\n",
    "features_test = pd.concat([ensemble_cols_test], axis=1)\n",
    "\n",
    "target_train, target_test = train[TARGET_LABEL_COL_NAME], test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "description = \"All Models' Uncertainties\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF & 1st Token Probability  (all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_feature_columns = retrieve_models_uncertainties_col_names(['first_token_probability'])\n",
    "\n",
    "ensemble_cols_train = pd.concat([train[col] for col in uncertainty_feature_columns], axis=1)\n",
    "features_train = pd.concat([ensemble_cols_train, textTrain], axis=1)\n",
    "\n",
    "ensemble_cols_test = pd.concat([test[col] for col in uncertainty_feature_columns], axis=1)\n",
    "features_test = pd.concat([ensemble_cols_test, textTest], axis=1)\n",
    "\n",
    "target_train, target_test = train[TARGET_LABEL_COL_NAME], test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "description = \"TF-IDF and All Models' 1st Token Probabilities\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF & Choice Order Probability  (all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_feature_columns = retrieve_models_uncertainties_col_names(['order_probability'])\n",
    "\n",
    "ensemble_cols_train = pd.concat([train[col] for col in uncertainty_feature_columns], axis=1)\n",
    "features_train = pd.concat([ensemble_cols_train, textTrain], axis=1)\n",
    "\n",
    "ensemble_cols_test = pd.concat([test[col] for col in uncertainty_feature_columns], axis=1)\n",
    "features_test = pd.concat([ensemble_cols_test, textTest], axis=1)\n",
    "\n",
    "target_train, target_test = train[TARGET_LABEL_COL_NAME], test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "description = \"TF-IDF and All Models' Choice Order Probabilities\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF & Both Uncertainties (all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_feature_columns = retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])\n",
    "\n",
    "ensemble_cols_train = pd.concat([train[col] for col in uncertainty_feature_columns], axis=1)\n",
    "features_train = pd.concat([ensemble_cols_train, textTrain], axis=1)\n",
    "\n",
    "ensemble_cols_test = pd.concat([test[col] for col in uncertainty_feature_columns], axis=1)\n",
    "features_test = pd.concat([ensemble_cols_test, textTest], axis=1)\n",
    "\n",
    "target_train, target_test = train[TARGET_LABEL_COL_NAME], test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "description = \"TF-IDF and All Models' Uncertainties\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Choice Similarity Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Choices similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHOICE_SIMILARITY_EXPERIMENTS:\n",
    "    similarity_to_use = \"choices_similarity\" # choices_similarity or choices_similarity_clinical !!!\n",
    "    # Concatenate the features\n",
    "    features_train = pd.concat([\n",
    "        train[similarity_to_use],\n",
    "    ], axis=1)\n",
    "    features_test = pd.concat([\n",
    "        test[similarity_to_use],\n",
    "    ], axis=1)\n",
    "\n",
    "    target_train, target_test = train[TARGET_LABEL_COL_NAME], test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "    description = \"Only \" + similarity_to_use\n",
    "    GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text, both uncertainties & Choices similarity (all models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHOICE_SIMILARITY_EXPERIMENTS:\n",
    "    uncertainty_feature_columns = retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])\n",
    "\n",
    "    # Concatenate the features\n",
    "    ensemble_cols_train = pd.concat([train[col] for col in uncertainty_feature_columns], axis=1)\n",
    "    features_train = pd.concat([ensemble_cols_train, train['choices_similarity'] , textTrain], axis=1)\n",
    "\n",
    "    ensemble_cols_test = pd.concat([test[col] for col in uncertainty_feature_columns], axis=1)\n",
    "    features_test = pd.concat([ensemble_cols_test, test['choices_similarity'] , textTest], axis=1)\n",
    "    target_train, target_test = train[TARGET_LABEL_COL_NAME], test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "    description = \"TF-IDF and All Models' Uncertainties and Choices Similarity\"\n",
    "    GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text, both uncertainties and Medical Choice Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHOICE_SIMILARITY_EXPERIMENTS:\n",
    "    uncertainty_feature_columns = retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])\n",
    "\n",
    "    # Concatenate the features\n",
    "    ensemble_cols_train = pd.concat([train[col] for col in uncertainty_feature_columns], axis=1)\n",
    "    features_train = pd.concat([ensemble_cols_train, train['choices_similarity_clinical'] , textTrain], axis=1)\n",
    "\n",
    "    ensemble_cols_test = pd.concat([test[col] for col in uncertainty_feature_columns], axis=1)\n",
    "    features_test = pd.concat([ensemble_cols_test, test['choices_similarity_clinical'] , textTest], axis=1)\n",
    "    target_train, target_test = train[TARGET_LABEL_COL_NAME], test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "    description = \"TF-IDF and All Models' Uncertainties and (clinical) Choices Similarity\"\n",
    "    GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text, both uncertainties and both Choice Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHOICE_SIMILARITY_EXPERIMENTS:\n",
    "    uncertainty_feature_columns = retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])\n",
    "\n",
    "    # Concatenate the features\n",
    "    ensemble_cols_train = pd.concat([train[col] for col in uncertainty_feature_columns], axis=1)\n",
    "    features_train = pd.concat([ensemble_cols_train, train['choices_similarity_clinical'], train['choices_similarity'] , textTrain], axis=1)\n",
    "\n",
    "    ensemble_cols_test = pd.concat([test[col] for col in uncertainty_feature_columns], axis=1)\n",
    "    features_test = pd.concat([ensemble_cols_test, test['choices_similarity_clinical'],test['choices_similarity'] , textTest], axis=1)\n",
    "    target_train, target_test = train[TARGET_LABEL_COL_NAME], test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "    description = \"TF-IDF and All Models' Uncertainties and both Choices Similarity\"\n",
    "    GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text, both uncertainties, both Choice Similarities and Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHOICE_SIMILARITY_EXPERIMENTS:\n",
    "    linguistic_features= [\"Word_Count\", \"Word_Count_No_stop_words\", \"Avg_Word_Length\", \"Sentence_Count\", \"Avg_Sent_Length_in_Words\", \"Noun_Count\", \"Verb_Count\", \"Adjective_Count\", \"Adverb_Count\", \"Number_of_NPs\", \"Number_of_PPs\", \"Number_of_VPs\", \"Temporal_Connectives_Count\", \"Causal_Connectives_Count\", \"Exemplifying_Connectives_Count\", \"Additive_Connectives_Count\", \"Contrastive_Connectives_Count\"]\n",
    "    \n",
    "    uncertainty_feature_columns = retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])\n",
    "\n",
    "    # Concatenate the features\n",
    "    ensemble_cols_train = pd.concat([train[col] for col in uncertainty_feature_columns], axis=1)\n",
    "    features_train = pd.concat([ensemble_cols_train, train['choices_similarity'], train['choices_similarity_clinical'], train_with_linguistic[linguistic_features], textTrain], axis=1)\n",
    "\n",
    "    ensemble_cols_test = pd.concat([test[col] for col in uncertainty_feature_columns], axis=1)\n",
    "    features_test = pd.concat([ensemble_cols_test, test['choices_similarity'], test['choices_similarity_clinical'], test_with_linguistic[linguistic_features], textTest], axis=1)\n",
    "    target_train, target_test = train[TARGET_LABEL_COL_NAME], test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "    description = \"TF-IDF, All Models' Uncertainties, All Choices Similarities \\n and Linguistic Features\"\n",
    "    GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Results RMSE Overview and Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we filter results to only the ones we are interested in\n",
    "checkboxes = [widgets.Checkbox(value=True, description=label, layout=widgets.Layout(\n",
    "    width='1000px')) for label in GLOBAL_ALL_RESULTS]\n",
    "output = widgets.VBox(children=checkboxes)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_keys = []\n",
    "for i in range(0, len(checkboxes)):\n",
    "    if checkboxes[i].value == True:\n",
    "        selected_keys = selected_keys + [checkboxes[i].description]\n",
    "\n",
    "for key in selected_keys:\n",
    "    print(key)\n",
    "    print(\"RMSE: \", GLOBAL_ALL_RESULTS[key][0]['rmse'], \"±\", GLOBAL_ALL_RESULTS[key][0]['std_error'], \" (STDEV: \", GLOBAL_ALL_RESULTS[key][0]['std_dev'], \")\")\n",
    "    print(\"--------------------\")\n",
    "    if key == 'DummyRegressor':\n",
    "        # there are no features to plot\n",
    "        continue\n",
    "    ### Plotting ###\n",
    "    # Set the font\n",
    "    plt.rcParams[\"font.family\"] = \"monospace\"\n",
    "    \n",
    "    # Create the main plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    GLOBAL_ALL_RESULTS[key][1].sort(key=lambda x: x[1]) # descending order by importance\n",
    "    ax.barh([x[0] for x in GLOBAL_ALL_RESULTS[key][1]], [x[1] for x in GLOBAL_ALL_RESULTS[key][1]], color='#478058')\n",
    "    \n",
    "    # Set title and labels\n",
    "    title_text = 'Feature Contributions to Model Performance\\n' + '(' + key + ')' # title text\n",
    "    fig.suptitle(title_text, ha='center', fontsize=20, y=1.01) # set title\n",
    "    ax.set_xlabel('Feature Importance', fontsize=18) # x label\n",
    "    # set y ticks manually, so they are within the barplot using plt.text\n",
    "    max_importance = max([x[1] for x in GLOBAL_ALL_RESULTS[key][1]])\n",
    "    for i, (feature, importance) in enumerate(GLOBAL_ALL_RESULTS[key][1]):\n",
    "        if importance > max_importance/2:\n",
    "            text_length = len(feature) * 0.002 # This is very hacky: manually tune this\n",
    "            ax.text(0.01, i, feature, ha='left', va='center', fontsize=10, color='white') # aligned to the right of the bar\n",
    "        else:\n",
    "            ax.text(0.0001+importance, i, feature, ha='left', va='center', fontsize=10, color='black') # might need to adjust the 0.0001\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    plt.show()\n",
    "    fig.savefig(f'plots/{DATASET}/{key}.png', dpi=200, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "difficulty_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
