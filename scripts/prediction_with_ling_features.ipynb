{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "# Sci-kit learn packages\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Packages for results/plotting\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import ticker\n",
    "from shap import summary_plot, TreeExplainer\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Change default font of matplotlib to monospace\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rcParams[\"font.family\"] = \"monospace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cmcqrd' # 'usmle', 'bio' or 'cmcqrd'\n",
    "RESULTS_DATASET = '../data/' + DATASET + '/with_ling_features/'\n",
    "TARGET_LABEL_COL_NAME = 'Correct_Answer_Rate' # \"Correct_Answer_Rate\" or \"Difficulty\" (only for cmcqrd) or \"Response_Time\"(for usmle only)\n",
    "REPETITIONS = 10  # Number of repetitions for each experiment, average will be taken\n",
    "# Number of cores to use for sklearn's n_jobs parameter, whenever possible\n",
    "NUM_OF_CORES_TO_USE = multiprocessing.cpu_count()\n",
    "print(\"Using \", NUM_OF_CORES_TO_USE, \" cores.\")\n",
    "# Number of most important features to print for the random forest model\n",
    "NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data. If the dataset with the cleaned text already exists, load it. Otherwise, create it (this may take a while).\n",
    "train = pd.read_csv(RESULTS_DATASET + 'train.csv')\n",
    "test = pd.read_csv(RESULTS_DATASET + 'test.csv', index_col=0)\n",
    "\n",
    "# Size per split\n",
    "print(\"Train size: \", len(train))\n",
    "print(\"Test size: \", len(test))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_ALL_RESULTS = {} # Store all results here\n",
    "\n",
    "def test_random_forest(features_train, target_train, features_test, target_test, description):\n",
    "    \"\"\"Runs a Random Forest model on the given data and returns the RMSE and the top features. Average of REPETITIONS is taken.\"\"\"\n",
    "    feature_importances_sum = None\n",
    "\n",
    "    all_rmses_for_model = []\n",
    "\n",
    "    # Initialize a sum array for feature importances\n",
    "    feature_importances_sum = np.zeros(features_train.shape[1])  # Number of features in the dataset)\n",
    "    shap_values = []\n",
    "    for repetition in tqdm(range(REPETITIONS)):\n",
    "        model = RandomForestRegressor(n_jobs=NUM_OF_CORES_TO_USE) # Re-initialize the model every time\n",
    "        # Fit the random forest. model.fit resets the model every time so it doesn't remember the previous fit.\n",
    "        model.fit(features_train, target_train)\n",
    "        # Predict the target values\n",
    "        predictions = model.predict(features_test)\n",
    "        rmse = root_mean_squared_error(predictions, target_test)  # Calculate RMSE\n",
    "        all_rmses_for_model.append(rmse)\n",
    "        # Collect feature importances\n",
    "        feature_importances_sum += model.feature_importances_\n",
    "        ### SHAP\n",
    "        explainer = TreeExplainer(model, approximate=True)\n",
    "        shap_values_of_repetition = explainer(features_train)\n",
    "        shap_values.append(shap_values_of_repetition)\n",
    "\n",
    "\n",
    "    # Calculate statistics for the model\n",
    "    average_rmse_for_model = float(np.mean(all_rmses_for_model)) # Mean RMSE\n",
    "    std_dev_rmse = float(np.std(all_rmses_for_model, ddof=1))  # Sample standard deviation\n",
    "    std_error_rmse = float(std_dev_rmse / np.sqrt(REPETITIONS))  # Standard error\n",
    "\n",
    "    # Store a summary of the results\n",
    "    rmse_results_summary = {\n",
    "        'rmse': round(average_rmse_for_model, 4),\n",
    "        'std_dev': round(std_dev_rmse, 4),\n",
    "        'std_error': round(std_error_rmse, 4)\n",
    "    }\n",
    "\n",
    "    # Print the top NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT important features for Random Forest\n",
    "    # Calculate the average feature importances across all repetitions\n",
    "    avg_feature_importances = feature_importances_sum / REPETITIONS\n",
    "    # Get the indices of the top NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT most important features\n",
    "    indices = np.argsort(avg_feature_importances)[-NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT:][::-1]\n",
    "    top_features_and_importances = [(features_train.columns[i], float(round(avg_feature_importances[i], 4))) for i in indices]\n",
    "    \n",
    "    # Average SHAP values over repetitions\n",
    "    shap_values = np.array([shap_values[i].values for i in range(REPETITIONS)])\n",
    "    shap_values = np.mean(shap_values, axis=0)\n",
    "    \n",
    "    # Summary plot:\n",
    "    summary_plot(shap_values, features_train, max_display=NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT, plot_size=[12, 5], show=False)\n",
    "    plt.suptitle('Effect of Top Features on Predicting Student Success \\n(' + description + ')', fontsize=20, x=0.5, y=1.1)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.xlabel('Impact on Random Forest Prediction', fontsize=18)\n",
    "    ax = plt.gca()  # Get current axis\n",
    "    ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f\"{x:.2g}\"))\n",
    "    plt.show()\n",
    "    plt.close()    \n",
    "    return rmse_results_summary, top_features_and_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_word2vec_model(sentences, vector_size=100, window=5, min_count=1, workers=4):\n",
    "    \"\"\"\n",
    "    Train a Word2Vec model on the given tokenized sentences.\n",
    "    \"\"\"\n",
    "    return Word2Vec(sentences=sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "\n",
    "def get_word2vec_emb(column, model):\n",
    "    \"\"\"\n",
    "    Get Word2Vec embeddings for a given column using a pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "        column (pd.Series): Column of text data.\n",
    "        model (Word2Vec): Pre-trained Word2Vec model.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame where each row is a vector representation of the text.\n",
    "    \"\"\"\n",
    "    def get_embedding(tokens):\n",
    "        # Extract embeddings for words in the model\n",
    "        valid_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "        if valid_vectors:\n",
    "            return np.mean(valid_vectors, axis=0)  # Compute average vector\n",
    "        else:\n",
    "            return np.zeros(model.vector_size)  # Use zero vector if no valid words\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = column.astype(str).apply(lambda x: x.split())\n",
    "\n",
    "    # Get embeddings for each row\n",
    "    embeddings_list = tokens.apply(get_embedding)\n",
    "\n",
    "    # Convert list of arrays into a DataFrame\n",
    "    return pd.DataFrame(embeddings_list.tolist(), index=column.index)\n",
    "\n",
    "# Tokenize sentences from both train and test\n",
    "all_sentences = train[\"question_with_options\"].astype(str).apply(lambda x: x.split()).tolist() + \\\n",
    "                test[\"question_with_options\"].astype(str).apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Train Word2Vec model once\n",
    "word2vec_model = train_word2vec_model(all_sentences)\n",
    "\n",
    "# Get embeddings for train and test\n",
    "train_word2vec = get_word2vec_emb(train[\"question_with_options\"], word2vec_model)\n",
    "test_word2vec = get_word2vec_emb(test[\"question_with_options\"], word2vec_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using All Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linguistic_features= [\"Word_Count\", \"Word_Count_No_stop_words\", \"Avg_Word_Length\", \"Sentence_Count\", \"Avg_Sent_Length_in_Words\", \"Noun_Count\", \"Verb_Count\", \"Adjective_Count\", \"Adverb_Count\", \"Number_of_NPs\", \"Number_of_PPs\", \"Number_of_VPs\", \"Temporal_Connectives_Count\", \"Causal_Connectives_Count\", \"Exemplifying_Connectives_Count\", \"Additive_Connectives_Count\", \"Contrastive_Connectives_Count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the features\n",
    "features_train = pd.concat([train[linguistic_features], train_word2vec], axis=1)\n",
    "features_train.columns = features_train.columns.astype(str)\n",
    "target_train = train[TARGET_LABEL_COL_NAME]\n",
    "features_test = pd.concat([test[linguistic_features], test_word2vec], axis=1)\n",
    "features_test.columns = features_test.columns.astype(str)\n",
    "target_test = test[TARGET_LABEL_COL_NAME]\n",
    "\n",
    "\n",
    "description = \"All Linguistic Features\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(features_train, target_train, features_test, target_test, description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Results RMSE Overview and Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we filter results to only the ones we are interested in\n",
    "checkboxes = [widgets.Checkbox(value=True, description=label, layout=widgets.Layout(\n",
    "    width='1000px')) for label in GLOBAL_ALL_RESULTS]\n",
    "output = widgets.VBox(children=checkboxes)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_keys = []\n",
    "for i in range(0, len(checkboxes)):\n",
    "    if checkboxes[i].value == True:\n",
    "        selected_keys = selected_keys + [checkboxes[i].description]\n",
    "\n",
    "for key in selected_keys:\n",
    "    print(key)\n",
    "    print(\"RMSE: \", GLOBAL_ALL_RESULTS[key][0]['rmse'], \"±\", GLOBAL_ALL_RESULTS[key][0]['std_error'], \" (STDEV: \", GLOBAL_ALL_RESULTS[key][0]['std_dev'], \")\")\n",
    "    print(\"--------------------\")\n",
    "    if key == 'DummyRegressor':\n",
    "        # there are no features to plot\n",
    "        continue\n",
    "    ### Plotting ###\n",
    "    # Set the font\n",
    "    plt.rcParams[\"font.family\"] = \"monospace\"\n",
    "    \n",
    "    # Create the main plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    GLOBAL_ALL_RESULTS[key][1].sort(key=lambda x: x[1]) # descending order by importance\n",
    "    ax.barh([x[0] for x in GLOBAL_ALL_RESULTS[key][1]], [x[1] for x in GLOBAL_ALL_RESULTS[key][1]], color='#478058')\n",
    "    \n",
    "    # Set title and labels\n",
    "    title_text = 'Feature Contributions to Model Performance\\n' + '(' + key + ')' # title text\n",
    "    fig.suptitle(title_text, ha='center', fontsize=20, y=1.01) # set title\n",
    "    ax.set_xlabel('Feature Importance', fontsize=18) # x label\n",
    "    max_importance = max([x[1] for x in GLOBAL_ALL_RESULTS[key][1]])\n",
    "    for i, (feature, importance) in enumerate(GLOBAL_ALL_RESULTS[key][1]):\n",
    "        if importance > max_importance/2:\n",
    "            text_length = len(feature) * 0.002 # This is very hacky: manually tune this\n",
    "            ax.text(0.01, i, feature, ha='left', va='center', fontsize=10, color='white') # aligned to the right of the bar\n",
    "        else:\n",
    "            ax.text(0.0001+importance, i, feature, ha='left', va='center', fontsize=10, color='black') # might need to adjust the 0.0001\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "difficulty_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
