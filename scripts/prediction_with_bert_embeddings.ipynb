{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "os.environ['HF_HOME'] = '/scratch/' + str(open('../tokens/HPC_ACCOUNT_ID.txt', 'r').read())\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Packages for results/plotting\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "# Change default font of matplotlib to monospace\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rcParams[\"font.family\"] = \"monospace\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cmcqrd' # 'usmle', 'bio' or 'cmcqrd'\n",
    "RESULTS_DATASET = '../data/' + DATASET + '/preprocessed/combined_results'\n",
    "TARGET_LABEL_COL_NAME = 'Correct_Answer_Rate' # \"Correct_Answer_Rate\" or \"Difficulty\" (latter only for cmcqrd) or \"Response_Time\"(for usmle only)\n",
    "REPETITIONS = 10 # Number of repetitions for each experiment, average will be taken\n",
    "EMBEDDINGS_MODEL = 'bert-base-uncased' # 'bert-base-uncased' or 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "HF_TOKEN = open('../tokens/HF_TOKEN.txt', 'r').read()\n",
    "# Number of cores to use for sklearn's n_jobs parameter, whenever possible\n",
    "NUM_OF_CORES_TO_USE = multiprocessing.cpu_count() \n",
    "print(\"Using \", NUM_OF_CORES_TO_USE, \" cores.\")\n",
    "\n",
    "MODEL_NAMES = ['phi3_5-chat', 'Llama3_2-3b-chat', 'Qwen2_5-3b-chat', 'Llama3_1-8b-chat', 'Qwen2_5-14b-chat', 'Qwen2_5-32b-chat', 'Yi-34b-chat', 'Qwen2_5-72b-chat', 'Llama3_1-70b-chat']\n",
    "\n",
    "# Number of most important features to print for the random forest model\n",
    "NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT = 10\n",
    "\n",
    "FULL_PRECISION_MODELS = False\n",
    "TEXT_COLUMNS_TO_EMBED = ['question_with_options']\n",
    "# Define the feature columns from the other models, that contain other uncertainty features\n",
    "\n",
    "def retrieve_models_uncertainties_col_names(metric_names):\n",
    "    uncertainty_feature_column_names = []\n",
    "    for metric in metric_names:\n",
    "        for model in MODEL_NAMES:\n",
    "            uncertainty_feature_column_names.append(f'{metric}_{model}')\n",
    "    return uncertainty_feature_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and predefined splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occasionally the columns are numbers, so we convert them to text strings\n",
    "def convert_cols_to_str(df, cols_names):\n",
    "    \"\"\"Convert columns to string type.\"\"\"\n",
    "    for col in cols_names:\n",
    "        df[col] = df[col].apply(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "train = None\n",
    "test = None\n",
    "\n",
    "if FULL_PRECISION_MODELS:\n",
    "    train = convert_cols_to_str(pd.read_csv(RESULTS_DATASET + '_fp_train_set.csv'), TEXT_COLUMNS_TO_EMBED)\n",
    "    test = convert_cols_to_str(pd.read_csv(RESULTS_DATASET + '_fp_test_set.csv', index_col=0), TEXT_COLUMNS_TO_EMBED)\n",
    "else:\n",
    "    train = convert_cols_to_str(pd.read_csv(RESULTS_DATASET + '_train_set.csv'), TEXT_COLUMNS_TO_EMBED)\n",
    "    test = convert_cols_to_str(pd.read_csv(RESULTS_DATASET + '_test_set.csv', index_col=0), TEXT_COLUMNS_TO_EMBED)\n",
    "\n",
    "train_with_linguistic = pd.read_csv('../data/' + DATASET + '/with_ling_features/' + 'train.csv')\n",
    "test_with_linguistic = pd.read_csv('../data/' + DATASET + '/with_ling_features/' + 'test.csv', index_col=0)\n",
    "\n",
    "# Size per split\n",
    "print(\"Train size: \", len(train))\n",
    "print(\"Test size: \", len(test))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Text Embeddings for the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text, model, tokenizer):\n",
    "    \"\"\"Extract BERT embeddings for a given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "    inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Get the [CLS] token embedding (typically at index 0) for each sequence\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return cls_embeddings\n",
    "\n",
    "def extract_combined_embeddings(df, text_columns):\n",
    "    \"\"\"Extract BERT embeddings for multiple text columns and concatenate them.\"\"\"\n",
    "    embeddings = []\n",
    "    for text_col in text_columns:\n",
    "        col_embeddings = np.vstack(df[text_col].apply(lambda x: get_bert_embeddings(x, bert_model, tokenizer)).values)\n",
    "        embeddings.append(col_embeddings)\n",
    "    # Combine embeddings from multiple text columns (concatenation)\n",
    "    combined_embeddings = np.hstack(embeddings)\n",
    "    return combined_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(EMBEDDINGS_MODEL, token = HF_TOKEN)\n",
    "bert_model = BertModel.from_pretrained(EMBEDDINGS_MODEL, token = HF_TOKEN)\n",
    "bert_model.eval()  # Set to evaluation mode to freeze the weights\n",
    "bert_model = bert_model.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the text embeddings for the train and test sets\n",
    "text_embeddings_train = extract_combined_embeddings(train, TEXT_COLUMNS_TO_EMBED)\n",
    "text_embeddings_test = extract_combined_embeddings(test, TEXT_COLUMNS_TO_EMBED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_ALL_RESULTS = {} # Store all results here\n",
    "\n",
    "def test_random_forest(features_train, target_train, features_test, target_test, description, features_label_list):\n",
    "    \"\"\"Runs a Random Forest model on the given data and returns the RMSE and the top features. Average of REPETITIONS is taken.\"\"\"\n",
    "    feature_importances_sum = None\n",
    "\n",
    "    all_rmses_for_model = []\n",
    "\n",
    "    # Initialize a sum array for feature importances\n",
    "    feature_importances_sum = np.zeros(features_train.shape[1])  # Number of features in the dataset)\n",
    "    for repetition in tqdm(range(REPETITIONS)):\n",
    "        model = RandomForestRegressor(n_jobs=NUM_OF_CORES_TO_USE) # Re-initialize the model every time\n",
    "        # Fit the random forest. model.fit resets the model every time so it doesn't remember the previous fit.\n",
    "        model.fit(features_train, target_train)\n",
    "        # Predict the target values\n",
    "        predictions = model.predict(features_test)\n",
    "        rmse = root_mean_squared_error(predictions, target_test)  # Calculate RMSE\n",
    "        all_rmses_for_model.append(rmse)\n",
    "        # Collect feature importances\n",
    "        feature_importances_sum += model.feature_importances_\n",
    "\n",
    "\n",
    "    # Calculate statistics for the model\n",
    "    average_rmse_for_model = float(np.mean(all_rmses_for_model)) #Â Mean RMSE\n",
    "    std_dev_rmse = float(np.std(all_rmses_for_model, ddof=1))  # Sample standard deviation\n",
    "    std_error_rmse = float(std_dev_rmse / np.sqrt(REPETITIONS))  # Standard error\n",
    "\n",
    "    # Store a summary of the results\n",
    "    rmse_results_summary = {\n",
    "        'rmse': round(average_rmse_for_model, 4),\n",
    "        'std_dev': round(std_dev_rmse, 4),\n",
    "        'std_error': round(std_error_rmse, 4)\n",
    "    }\n",
    "\n",
    "    # Print the top NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT important features for Random Forest\n",
    "    # Calculate the average feature importances across all repetitions\n",
    "    avg_feature_importances = feature_importances_sum / REPETITIONS    \n",
    "    indices = np.argsort(avg_feature_importances)[-NUMBER_OF_IMPORTANT_FEATURES_TO_PRINT:][::-1]\n",
    "    top_features_and_importances = [(features_label_list[i], float(round(avg_feature_importances[i], 4))) for i in indices]\n",
    "    \n",
    "    return rmse_results_summary, top_features_and_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only text embeddings\n",
    "X_train = text_embeddings_train\n",
    "X_test = text_embeddings_test\n",
    "y_train = train[TARGET_LABEL_COL_NAME].values\n",
    "y_test = test[TARGET_LABEL_COL_NAME].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "features_label_list = ['EMB_' + str(i) for i in range(X_train.shape[1])]\n",
    "description = \"Only BERT embeddings\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(X_train_scaled, y_train, X_test_scaled, y_test, description, features_label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embeddings & 1st Token Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uncertainties = train[retrieve_models_uncertainties_col_names(['first_token_probability'])].values\n",
    "test_uncertainties = test[retrieve_models_uncertainties_col_names(['first_token_probability'])].values\n",
    "\n",
    "X_train = np.hstack([text_embeddings_train, train_uncertainties])\n",
    "X_test = np.hstack([text_embeddings_test, test_uncertainties])\n",
    "\n",
    "y_train = train[TARGET_LABEL_COL_NAME].values\n",
    "y_test = test[TARGET_LABEL_COL_NAME].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "features_label_list = ['EMB_' + str(i) for i in range(text_embeddings_train.shape[1])]\n",
    "features_label_list += retrieve_models_uncertainties_col_names(['first_token_probability'])\n",
    "\n",
    "description = \"BERT Embeddings and All Models' 1st Token Probabilities\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(X_train_scaled, y_train, X_test_scaled, y_test, description, features_label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embeddings & Choice Order Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uncertainties = train[retrieve_models_uncertainties_col_names(['order_probability'])].values\n",
    "test_uncertainties = test[retrieve_models_uncertainties_col_names(['order_probability'])].values\n",
    "\n",
    "X_train = np.hstack([text_embeddings_train, train_uncertainties])\n",
    "X_test = np.hstack([text_embeddings_test, test_uncertainties])\n",
    "\n",
    "y_train = train[TARGET_LABEL_COL_NAME].values\n",
    "y_test = test[TARGET_LABEL_COL_NAME].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "features_label_list = ['EMB_' + str(i) for i in range(text_embeddings_train.shape[1])]\n",
    "features_label_list += retrieve_models_uncertainties_col_names(['order_probability'])\n",
    "\n",
    "description = \"BERT Embeddings and All Models' Choice Order Probabilities\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(X_train_scaled, y_train, X_test_scaled, y_test, description, features_label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embeddings, 1st Token Probability and Choice Order Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uncertainties = train[retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])].values\n",
    "test_uncertainties = test[retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])].values\n",
    "\n",
    "X_train = np.hstack([text_embeddings_train, train_uncertainties])\n",
    "X_test = np.hstack([text_embeddings_test, test_uncertainties])\n",
    "\n",
    "y_train = train[TARGET_LABEL_COL_NAME].values\n",
    "y_test = test[TARGET_LABEL_COL_NAME].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "features_label_list = ['EMB_' + str(i) for i in range(text_embeddings_train.shape[1])]\n",
    "features_label_list += retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])\n",
    "\n",
    "description = \"BERT Embeddings and All Models' Uncertainties\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(X_train_scaled, y_train, X_test_scaled, y_test, description, features_label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embeddings, 1st Token Probability, Choice Order Probability and  all-MiniLM-L6-v2 Choice Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uncertainties = train[retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])].values\n",
    "test_uncertainties = test[retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])].values\n",
    "\n",
    "X_train = np.hstack([text_embeddings_train, train_uncertainties])\n",
    "X_test = np.hstack([text_embeddings_test, test_uncertainties])\n",
    "\n",
    "# Add choices similarity\n",
    "X_train = np.hstack([X_train, train['choices_similarity'].values.reshape(-1, 1)])\n",
    "X_test = np.hstack([X_test, test['choices_similarity'].values.reshape(-1, 1)])\n",
    "\n",
    "y_train = train[TARGET_LABEL_COL_NAME].values\n",
    "y_test = test[TARGET_LABEL_COL_NAME].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "features_label_list = ['EMB_' + str(i) for i in range(text_embeddings_train.shape[1])]\n",
    "features_label_list += retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])\n",
    "features_label_list += ['Choices Similarity']\n",
    "\n",
    "description = \"BERT Embeddings and All Models' Uncertainties and Choices Similarity\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(X_train_scaled, y_train, X_test_scaled, y_test, description, features_label_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Medical Choice Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['choices_similarity_clinical'].values.reshape(-1, 1)\n",
    "X_test = test['choices_similarity_clinical'].values.reshape(-1, 1)\n",
    "\n",
    "y_train = train[TARGET_LABEL_COL_NAME].values\n",
    "y_test = test[TARGET_LABEL_COL_NAME].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "features_label_list = ['EMB_' + str(i) for i in range(text_embeddings_train.shape[1])]\n",
    "features_label_list += retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])\n",
    "features_label_list += ['Choices Similarity (Clinical)']\n",
    "\n",
    "description = \"BERT Embeddings and All Models' Uncertainties and (clinical) Choices Similarity\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(X_train_scaled, y_train, X_test_scaled, y_test, description, features_label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings and All Models' Uncertainties and (clinical) Choices Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uncertainties = train[retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])].values\n",
    "test_uncertainties = test[retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])].values\n",
    "\n",
    "X_train = np.hstack([text_embeddings_train, train_uncertainties])\n",
    "X_test = np.hstack([text_embeddings_test, test_uncertainties])\n",
    "\n",
    "# Add choices similarity\n",
    "X_train = np.hstack([X_train, train['choices_similarity_clinical'].values.reshape(-1, 1)])\n",
    "X_test = np.hstack([X_test, test['choices_similarity_clinical'].values.reshape(-1, 1)])\n",
    "\n",
    "y_train = train[TARGET_LABEL_COL_NAME].values\n",
    "y_test = test[TARGET_LABEL_COL_NAME].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "features_label_list = ['EMB_' + str(i) for i in range(text_embeddings_train.shape[1])]\n",
    "features_label_list += retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])\n",
    "features_label_list += ['Choices Similarity (Clinical)']\n",
    "\n",
    "description = \"BERT Embeddings and All Models' Uncertainties and (clinical) Choices Similarity\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(X_train_scaled, y_train, X_test_scaled, y_test, description, features_label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embeddings, Both Uncertainties & both similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uncertainties = train[retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])].values\n",
    "test_uncertainties = test[retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])].values\n",
    "\n",
    "X_train = np.hstack([text_embeddings_train, train_uncertainties])\n",
    "X_test = np.hstack([text_embeddings_test, test_uncertainties])\n",
    "\n",
    "# Add choices similarity\n",
    "X_train = np.hstack([X_train, train['choices_similarity_clinical'].values.reshape(-1, 1), train['choices_similarity'].values.reshape(-1, 1)])\n",
    "X_test = np.hstack([X_test, test['choices_similarity_clinical'].values.reshape(-1, 1), test['choices_similarity'].values.reshape(-1, 1)])\n",
    "\n",
    "y_train = train[TARGET_LABEL_COL_NAME].values\n",
    "y_test = test[TARGET_LABEL_COL_NAME].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "features_label_list = ['EMB_' + str(i) for i in range(text_embeddings_train.shape[1])]\n",
    "features_label_list += retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])\n",
    "features_label_list += ['Choices Similarity Clinical', 'Choices Similarity']\n",
    "\n",
    "description = \"BERT Embeddings, Both Uncertainties & Both similarities\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(X_train_scaled, y_train, X_test_scaled, y_test, description, features_label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Embeddings, Both Uncertainties, both similarities & Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uncertainties = train[retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])].values\n",
    "test_uncertainties = test[retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])].values\n",
    "\n",
    "linguistic_features= [\"Word_Count\", \"Word_Count_No_stop_words\", \"Avg_Word_Length\", \"Sentence_Count\", \"Avg_Sent_Length_in_Words\", \"Noun_Count\", \"Verb_Count\", \"Adjective_Count\", \"Adverb_Count\", \"Number_of_NPs\", \"Number_of_PPs\", \"Number_of_VPs\", \"Temporal_Connectives_Count\", \"Causal_Connectives_Count\", \"Exemplifying_Connectives_Count\", \"Additive_Connectives_Count\", \"Contrastive_Connectives_Count\"]\n",
    "\n",
    "X_train = np.hstack([text_embeddings_train, train_uncertainties, train_with_linguistic[linguistic_features]])\n",
    "X_test = np.hstack([text_embeddings_test, test_uncertainties, test_with_linguistic[linguistic_features]])\n",
    "\n",
    "# Add choices similarity\n",
    "X_train = np.hstack([X_train, train['choices_similarity_clinical'].values.reshape(-1, 1), train['choices_similarity'].values.reshape(-1, 1)])\n",
    "X_test = np.hstack([X_test, test['choices_similarity_clinical'].values.reshape(-1, 1), test['choices_similarity'].values.reshape(-1, 1)])\n",
    "\n",
    "y_train = train[TARGET_LABEL_COL_NAME].values\n",
    "y_test = test[TARGET_LABEL_COL_NAME].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "features_label_list = ['EMB_' + str(i) for i in range(text_embeddings_train.shape[1])]\n",
    "features_label_list += retrieve_models_uncertainties_col_names(['first_token_probability', 'order_probability'])\n",
    "features_label_list += linguistic_features\n",
    "features_label_list += ['Choices Similarity Clinical', 'Choices Similarity']\n",
    "\n",
    "description = \"BERT Embeddings, Both Uncertainties, both similarities & Linguistic Features\"\n",
    "GLOBAL_ALL_RESULTS[description] = test_random_forest(X_train_scaled, y_train, X_test_scaled, y_test, description, features_label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results RMSE Overview and Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we filter results to only the ones we are interested in\n",
    "checkboxes = [widgets.Checkbox(value=True, description=label, layout=widgets.Layout(\n",
    "    width='1000px')) for label in GLOBAL_ALL_RESULTS]\n",
    "output = widgets.VBox(children=checkboxes)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_keys = []\n",
    "for i in range(0, len(checkboxes)):\n",
    "    if checkboxes[i].value == True:\n",
    "        selected_keys = selected_keys + [checkboxes[i].description]\n",
    "\n",
    "for key in selected_keys:\n",
    "    print(key)\n",
    "    print(\"RMSE: \", GLOBAL_ALL_RESULTS[key][0]['rmse'], \"Â±\", GLOBAL_ALL_RESULTS[key][0]['std_error'], \" (STDEV: \", GLOBAL_ALL_RESULTS[key][0]['std_dev'], \")\")\n",
    "    print(\"--------------------\")\n",
    "    ### Plotting ###\n",
    "    plt.rcParams[\"font.family\"] = \"monospace\"\n",
    "    \n",
    "    # Create the main plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    GLOBAL_ALL_RESULTS[key][1].sort(key=lambda x: x[1]) # descending order by importance\n",
    "    ax.barh([x[0] for x in GLOBAL_ALL_RESULTS[key][1]], [x[1] for x in GLOBAL_ALL_RESULTS[key][1]], color='#478058')\n",
    "    \n",
    "    # Set title and labels\n",
    "    title_text = 'Feature Contributions to Model Performance\\n' + '(' + key + ')' # title text\n",
    "    fig.suptitle(title_text, ha='center', fontsize=20, y=1.01) # set title\n",
    "    ax.set_xlabel('Feature Importance', fontsize=18) # x label\n",
    "    # set y ticks manually, so they are within the barplot using plt.text\n",
    "    max_importance = max([x[1] for x in GLOBAL_ALL_RESULTS[key][1]])\n",
    "    for i, (feature, importance) in enumerate(GLOBAL_ALL_RESULTS[key][1]):\n",
    "        if importance > max_importance/2:\n",
    "            text_length = len(str(feature)) * 0.002 # This is very hacky: manually tune this\n",
    "            ax.text(0.01, i, feature, ha='left', va='center', fontsize=10, color='white') # aligned to the right of the bar\n",
    "        else:\n",
    "            ax.text(0.0001+importance, i, feature, ha='left', va='center', fontsize=10, color='black') # might need to adjust the 0.0001\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    # Save and show the plot\n",
    "    plt.show()\n",
    "    fig.savefig(f'plots/{DATASET}/{key}.png', dpi=200, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "difficulty_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
